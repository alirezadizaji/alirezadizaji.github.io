- name: GNN Explainability
  image: images/tum_gnn.png
  description: |
    ## **How to Evaluate Your Graph Neural Network Explanation by Retraining**
    
    ---
    Currently, several metrics have been proposed to evaluate the faithfulness of current state-of-the-art explanation methods
    in graph neural networks. However, there are several drawbacks among these evaluation metrics 
    that make their fair comparison challenging. Through this project, we mentioned these drawbacks 
    and instead proposed novel solutions with a guideline assisting in analyzing the focus of these 
    explainers better, and the target Graph Neural Networks.



    <span style="color:#6b0303"> Ashkan Khakzar, **Alireza Dizaji**, Razieh Rezaei, Anees Kazi, Nassir Navab, Daniel Rueckert</span> 
    <br><br>


- name: LAP
  image: images/lap.png
  description: |
    ## **LAP:An Attention-Based Module for Faithful Interpretation and Knowledge Injection in Convolutional Neural Networks**
    
    ---
    Through this project, we devised a self-interpretable model which could be easily injected into any CNN network.
    The method is a novel inspiration of both pooling and attention-based methods representing promising results 
    on many experiments passing state-of-the-art methods.
    
    
    <span style="color:#6b0303"> Rassa Ghavami Modegh, Ahmad Salimi, **Alireza Dizaji**, Hamid R. Rabiee</span> 
    <br><br>


